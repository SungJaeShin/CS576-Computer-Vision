{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Final] CS576 Assignment #1: BoVW classification (2022S)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfG_todxMT6c"
      },
      "source": [
        "CS576 Assignment #1: Image Classification using Bag of Visual Words (BoVW) \n",
        "====\n",
        "Primary TA : Whie Jung \n",
        "\n",
        "TA's E-mail : whieya@kaist.ac.kr, jinwoo-kim@kaist.ac.kr, seongwoongjo@kaist.ac.kr\n",
        "\n",
        "QnA Channel: \"assignment1\" channel in the SLACK channel (https://join.slack.com/t/kaist-cs576-cvhq/shared_invite/zt-15g6flwkk-lEIIBYFg8N0VRPHOsxhVrA)\n",
        "\n",
        "## Instruction\n",
        "- In this assignment, we will classify the images into five categories (aeroplane, backgrounds, car, horse, motorcycle, person) using Bag of Visual Word (BoVW) and Support Vector Machine (SVM).\n",
        " \n",
        "- We will extract the SIFT descriptors from the images and construct a codebook. After that, we will encode the images to histogram features using codebook, and train the classifier using those features.\n",
        "\n",
        "- As you follow the given steps, fill in the section marked ***Problem*** with the appropriate code. There are **7 problems** in total.\n",
        "    - For **Problem 1 ~ Problem 4**, you will get full credits (10pt each) if you implement correctly.  \n",
        "    - For **Problem 5 ~ Problem 7**, you **have to write a discussion about the results** as well as implementing the codes. Each problem takes 5pt for the correct implementation and 5 pt for proper discussion. In other words, you will get only 5pt without proper discussion even if you correctly implement the codes. To get full credit for discussion, please follow **Discussion Guidelines**.\n",
        "\n",
        "## Discussion Guidelines\n",
        "- You should write a discussion about **Problem 5 ~ Problem 7** on the **Discussion and Analysis** section. \n",
        "- Simply reporting the scores (e.g. classification accuracy) is not considered as a discussion.\n",
        "- For each problem's discussion, you should explain and compare how each method improves the results. \n",
        "\n",
        "## Submission guidelines\n",
        "- Your code and report will be all in Colab. Copy this example to your google drive and edit it to complete your assignment. \n",
        "- <font color=\"red\"> You will get the full credit **only if** you complete the code **and** write a discussion of the results in the discussion section at the bottom of this page. </font>\n",
        "- We should be able to reproduce your results using your code. Please double-check if your code runs without error and reproduces your results. Submissions failed to run or reproduce the results will get a substantial penalty. \n",
        "- <font color=\"red\"> **DO NOT modify any of the skeleton codes when you submit.** Please write your codes only in the designated area. </font>\n",
        "- As a proof that you've ran this code by yourself, **make sure your notebook contains the output of each code block.**\n",
        "\n",
        "## Deliverables\n",
        "- Download your Colab notebook, and submit it in a format: [StudentID].ipynb.\n",
        "- Your assignment should be submitted through KLMS. All other submissions (e.g., via email) will not be considered as valid submissions. \n",
        "\n",
        "## Due date\n",
        "- **23:59:59 April 6th.**\n",
        "- Late submission is allowed until 23:59:59 April 8th.\n",
        "- Late submission will be applied 20% penalty.\n",
        "\n",
        "\n",
        "\n",
        "## Questions\n",
        "- Please use \"assignment1\" channel in the SLACK channel (https://join.slack.com/t/kaist-cs576-cvhq/shared_invite/zt-15g6flwkk-lEIIBYFg8N0VRPHOsxhVrA) as a main communication channel. When you post questions, please make it public so that all students can share the information.\n",
        "- When you post questions, please avoid posting your own implementation (eg, posting the capture image of your own implementation.) \n",
        "\n",
        "## Modification\n",
        "- Descriptions of the `get_codebook` and `extract_features` are fixed. Shape of the parameter `des` in get_codebook function and extract_features function should be [num_images, ], not [num_images, num_des_of_each_img, 128]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RysBzJQFeIkg"
      },
      "source": [
        "## Step 0: Set the enviroments\n",
        "For this assignment, you need the special library for extracting features & training classifier (cyvlfeat & sklearn).\n",
        "This step takes about 5~15 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26drrtRufRbK"
      },
      "source": [
        "###  0-1: Download cyvlfeat library & conda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEjDierhsAZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcbe47d7-738c-4ef5-c23f-fb59e655677b"
      },
      "source": [
        "# install conda on colab\n",
        "!wget -c https://repo.continuum.io/archive/Anaconda3-5.3.1-Linux-x86_64.sh\n",
        "!chmod +x Anaconda3-5.3.1-Linux-x86_64.sh\n",
        "!bash ./Anaconda3-5.3.1-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "# install cyvlfeat\n",
        "# Reference : https://anaconda.org/menpo/cyvlfeat\n",
        "!conda install -c menpo cyvlfeat python==3.7 -y\n",
        "!conda install cython numpy scipy -y\n",
        "\n",
        "import sys\n",
        "sys.path.append('/cyvlfeat')\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
        "\n",
        "!git clone https://github.com/menpo/cyvlfeat.git /cyvlfeat\n",
        "!cd /cyvlfeat && CFLAGS=\"-I$CONDA_PREFIX/include\" LDFLAGS=\"-L$CONDA_PREFIX/lib\" pip install -e ./"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-05 09:56:50--  https://repo.continuum.io/archive/Anaconda3-5.3.1-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.201.79, 104.18.200.79, 2606:4700::6812:c84f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.201.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://repo.anaconda.com/archive/Anaconda3-5.3.1-Linux-x86_64.sh [following]\n",
            "--2022-04-05 09:56:50--  https://repo.anaconda.com/archive/Anaconda3-5.3.1-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 667976437 (637M) [application/x-sh]\n",
            "Saving to: ‘Anaconda3-5.3.1-Linux-x86_64.sh’\n",
            "\n",
            "Anaconda3-5.3.1-Lin 100%[===================>] 637.03M   146MB/s    in 4.5s    \n",
            "\n",
            "2022-04-05 09:56:55 (142 MB/s) - ‘Anaconda3-5.3.1-Linux-x86_64.sh’ saved [667976437/667976437]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "reinstalling: python-3.7.0-hc3d631a_0 ...\n",
            "Python 3.7.0\n",
            "reinstalling: blas-1.0-mkl ...\n",
            "reinstalling: ca-certificates-2018.03.07-0 ...\n",
            "reinstalling: conda-env-2.6.0-1 ...\n",
            "reinstalling: intel-openmp-2019.0-118 ...\n",
            "reinstalling: libgcc-ng-8.2.0-hdf63c60_1 ...\n",
            "reinstalling: libgfortran-ng-7.3.0-hdf63c60_0 ...\n",
            "reinstalling: libstdcxx-ng-8.2.0-hdf63c60_1 ...\n",
            "reinstalling: bzip2-1.0.6-h14c3975_5 ...\n",
            "reinstalling: expat-2.2.6-he6710b0_0 ...\n",
            "reinstalling: fribidi-1.0.5-h7b6447c_0 ...\n",
            "reinstalling: gmp-6.1.2-h6c8ec71_1 ...\n",
            "reinstalling: graphite2-1.3.12-h23475e2_2 ...\n",
            "reinstalling: icu-58.2-h9c2bf20_1 ...\n",
            "reinstalling: jbig-2.1-hdba287a_0 ...\n",
            "reinstalling: jpeg-9b-h024ee3a_2 ...\n",
            "reinstalling: libffi-3.2.1-hd88cf55_4 ...\n",
            "reinstalling: libsodium-1.0.16-h1bed415_0 ...\n",
            "reinstalling: libtool-2.4.6-h544aabb_3 ...\n",
            "reinstalling: libuuid-1.0.3-h1bed415_2 ...\n",
            "reinstalling: libxcb-1.13-h1bed415_1 ...\n",
            "reinstalling: lzo-2.10-h49e0be7_2 ...\n",
            "reinstalling: mkl-2019.0-118 ...\n",
            "reinstalling: ncurses-6.1-hf484d3e_0 ...\n",
            "reinstalling: openssl-1.0.2p-h14c3975_0 ...\n",
            "reinstalling: patchelf-0.9-hf484d3e_2 ...\n",
            "reinstalling: pcre-8.42-h439df22_0 ...\n",
            "reinstalling: pixman-0.34.0-hceecf20_3 ...\n",
            "reinstalling: snappy-1.1.7-hbae5bb6_3 ...\n",
            "reinstalling: xz-5.2.4-h14c3975_4 ...\n",
            "reinstalling: yaml-0.1.7-had09818_2 ...\n",
            "reinstalling: zlib-1.2.11-ha838bed_2 ...\n",
            "reinstalling: blosc-1.14.4-hdbcaa40_0 ...\n",
            "reinstalling: glib-2.56.2-hd408876_0 ...\n",
            "reinstalling: hdf5-1.10.2-hba1933b_1 ...\n",
            "reinstalling: libedit-3.1.20170329-h6b74fdf_2 ...\n",
            "reinstalling: libpng-1.6.34-hb9fc6fc_0 ...\n",
            "reinstalling: libssh2-1.8.0-h9cfc8f7_4 ...\n",
            "reinstalling: libtiff-4.0.9-he85c1e1_2 ...\n",
            "reinstalling: libxml2-2.9.8-h26e45fe_1 ...\n",
            "reinstalling: mpfr-4.0.1-hdf1c602_3 ...\n",
            "reinstalling: pandoc-1.19.2.1-hea2e7c5_1 ...\n",
            "reinstalling: readline-7.0-h7b6447c_5 ...\n",
            "reinstalling: tk-8.6.8-hbc83047_0 ...\n",
            "reinstalling: zeromq-4.2.5-hf484d3e_1 ...\n",
            "reinstalling: dbus-1.13.2-h714fa37_1 ...\n",
            "reinstalling: freetype-2.9.1-h8a8886c_1 ...\n",
            "reinstalling: gstreamer-1.14.0-hb453b48_1 ...\n",
            "reinstalling: libcurl-7.61.0-h1ad7b7a_0 ...\n",
            "reinstalling: libxslt-1.1.32-h1312cb7_0 ...\n",
            "reinstalling: mpc-1.1.0-h10f8cd9_1 ...\n",
            "reinstalling: sqlite-3.24.0-h84994c4_0 ...\n",
            "reinstalling: unixodbc-2.3.7-h14c3975_0 ...\n",
            "reinstalling: curl-7.61.0-h84994c4_0 ...\n",
            "reinstalling: fontconfig-2.13.0-h9420a91_0 ...\n",
            "reinstalling: gst-plugins-base-1.14.0-hbbd80ab_1 ...\n",
            "reinstalling: alabaster-0.7.11-py37_0 ...\n",
            "reinstalling: appdirs-1.4.3-py37h28b3542_0 ...\n",
            "reinstalling: asn1crypto-0.24.0-py37_0 ...\n",
            "reinstalling: atomicwrites-1.2.1-py37_0 ...\n",
            "reinstalling: attrs-18.2.0-py37h28b3542_0 ...\n",
            "reinstalling: backcall-0.1.0-py37_0 ...\n",
            "reinstalling: backports-1.0-py37_1 ...\n",
            "reinstalling: beautifulsoup4-4.6.3-py37_0 ...\n",
            "reinstalling: bitarray-0.8.3-py37h14c3975_0 ...\n",
            "reinstalling: boto-2.49.0-py37_0 ...\n",
            "reinstalling: cairo-1.14.12-h8948797_3 ...\n",
            "reinstalling: certifi-2018.8.24-py37_1 ...\n",
            "reinstalling: chardet-3.0.4-py37_1 ...\n",
            "reinstalling: click-6.7-py37_0 ...\n",
            "reinstalling: cloudpickle-0.5.5-py37_0 ...\n",
            "reinstalling: colorama-0.3.9-py37_0 ...\n",
            "reinstalling: constantly-15.1.0-py37h28b3542_0 ...\n",
            "reinstalling: contextlib2-0.5.5-py37_0 ...\n",
            "reinstalling: dask-core-0.19.1-py37_0 ...\n",
            "reinstalling: decorator-4.3.0-py37_0 ...\n",
            "reinstalling: defusedxml-0.5.0-py37_1 ...\n",
            "reinstalling: docutils-0.14-py37_0 ...\n",
            "reinstalling: entrypoints-0.2.3-py37_2 ...\n",
            "reinstalling: et_xmlfile-1.0.1-py37_0 ...\n",
            "reinstalling: fastcache-1.0.2-py37h14c3975_2 ...\n",
            "reinstalling: filelock-3.0.8-py37_0 ...\n",
            "reinstalling: glob2-0.6-py37_0 ...\n",
            "reinstalling: gmpy2-2.0.8-py37h10f8cd9_2 ...\n",
            "reinstalling: greenlet-0.4.15-py37h7b6447c_0 ...\n",
            "reinstalling: heapdict-1.0.0-py37_2 ...\n",
            "reinstalling: idna-2.7-py37_0 ...\n",
            "reinstalling: imagesize-1.1.0-py37_0 ...\n",
            "reinstalling: incremental-17.5.0-py37_0 ...\n",
            "reinstalling: ipython_genutils-0.2.0-py37_0 ...\n",
            "reinstalling: itsdangerous-0.24-py37_1 ...\n",
            "reinstalling: jdcal-1.4-py37_0 ...\n",
            "reinstalling: jeepney-0.3.1-py37_0 ...\n",
            "reinstalling: kiwisolver-1.0.1-py37hf484d3e_0 ...\n",
            "reinstalling: lazy-object-proxy-1.3.1-py37h14c3975_2 ...\n",
            "reinstalling: llvmlite-0.24.0-py37hdbcaa40_0 ...\n",
            "reinstalling: locket-0.2.0-py37_1 ...\n",
            "reinstalling: lxml-4.2.5-py37hefd8a0e_0 ...\n",
            "reinstalling: markupsafe-1.0-py37h14c3975_1 ...\n",
            "reinstalling: mccabe-0.6.1-py37_1 ...\n",
            "reinstalling: mistune-0.8.3-py37h14c3975_1 ...\n",
            "reinstalling: mkl-service-1.1.2-py37h90e4bf4_5 ...\n",
            "reinstalling: mpmath-1.0.0-py37_2 ...\n",
            "reinstalling: msgpack-python-0.5.6-py37h6bb024c_1 ...\n",
            "reinstalling: numpy-base-1.15.1-py37h81de0dd_0 ...\n",
            "reinstalling: olefile-0.46-py37_0 ...\n",
            "reinstalling: pandocfilters-1.4.2-py37_1 ...\n",
            "reinstalling: parso-0.3.1-py37_0 ...\n",
            "reinstalling: path.py-11.1.0-py37_0 ...\n",
            "reinstalling: pep8-1.7.1-py37_0 ...\n",
            "reinstalling: pickleshare-0.7.4-py37_0 ...\n",
            "reinstalling: pkginfo-1.4.2-py37_1 ...\n",
            "reinstalling: pluggy-0.7.1-py37h28b3542_0 ...\n",
            "reinstalling: ply-3.11-py37_0 ...\n",
            "reinstalling: psutil-5.4.7-py37h14c3975_0 ...\n",
            "reinstalling: ptyprocess-0.6.0-py37_0 ...\n",
            "reinstalling: py-1.6.0-py37_0 ...\n",
            "reinstalling: pyasn1-0.4.4-py37h28b3542_0 ...\n",
            "reinstalling: pycodestyle-2.4.0-py37_0 ...\n",
            "reinstalling: pycosat-0.6.3-py37h14c3975_0 ...\n",
            "reinstalling: pycparser-2.18-py37_1 ...\n",
            "reinstalling: pycrypto-2.6.1-py37h14c3975_9 ...\n",
            "reinstalling: pycurl-7.43.0.2-py37hb7f436b_0 ...\n",
            "reinstalling: pyflakes-2.0.0-py37_0 ...\n",
            "reinstalling: pyodbc-4.0.24-py37he6710b0_0 ...\n",
            "reinstalling: pyparsing-2.2.0-py37_1 ...\n",
            "reinstalling: pysocks-1.6.8-py37_0 ...\n",
            "reinstalling: pytz-2018.5-py37_0 ...\n",
            "reinstalling: pyyaml-3.13-py37h14c3975_0 ...\n",
            "reinstalling: pyzmq-17.1.2-py37h14c3975_0 ...\n",
            "reinstalling: qt-5.9.6-h8703b6f_2 ...\n",
            "reinstalling: qtpy-1.5.0-py37_0 ...\n",
            "reinstalling: rope-0.11.0-py37_0 ...\n",
            "reinstalling: ruamel_yaml-0.15.46-py37h14c3975_0 ...\n",
            "reinstalling: send2trash-1.5.0-py37_0 ...\n",
            "reinstalling: simplegeneric-0.8.1-py37_2 ...\n",
            "reinstalling: sip-4.19.8-py37hf484d3e_0 ...\n",
            "reinstalling: six-1.11.0-py37_1 ...\n",
            "reinstalling: snowballstemmer-1.2.1-py37_0 ...\n",
            "reinstalling: sortedcontainers-2.0.5-py37_0 ...\n",
            "reinstalling: sphinxcontrib-1.0-py37_1 ...\n",
            "reinstalling: sqlalchemy-1.2.11-py37h7b6447c_0 ...\n",
            "reinstalling: tblib-1.3.2-py37_0 ...\n",
            "reinstalling: testpath-0.3.1-py37_0 ...\n",
            "reinstalling: toolz-0.9.0-py37_0 ...\n",
            "reinstalling: tornado-5.1-py37h14c3975_0 ...\n",
            "reinstalling: tqdm-4.26.0-py37h28b3542_0 ...\n",
            "reinstalling: unicodecsv-0.14.1-py37_0 ...\n",
            "reinstalling: wcwidth-0.1.7-py37_0 ...\n",
            "reinstalling: webencodings-0.5.1-py37_1 ...\n",
            "reinstalling: werkzeug-0.14.1-py37_0 ...\n",
            "reinstalling: wrapt-1.10.11-py37h14c3975_2 ...\n",
            "reinstalling: xlrd-1.1.0-py37_1 ...\n",
            "reinstalling: xlsxwriter-1.1.0-py37_0 ...\n",
            "reinstalling: xlwt-1.3.0-py37_0 ...\n",
            "reinstalling: zope-1.0-py37_1 ...\n",
            "reinstalling: astroid-2.0.4-py37_0 ...\n",
            "reinstalling: automat-0.7.0-py37_0 ...\n",
            "reinstalling: babel-2.6.0-py37_0 ...\n",
            "reinstalling: backports.shutil_get_terminal_size-1.0.0-py37_2 ...\n",
            "reinstalling: cffi-1.11.5-py37he75722e_1 ...\n",
            "reinstalling: cycler-0.10.0-py37_0 ...\n",
            "reinstalling: cytoolz-0.9.0.1-py37h14c3975_1 ...\n",
            "reinstalling: harfbuzz-1.8.8-hffaf4a1_0 ...\n",
            "reinstalling: html5lib-1.0.1-py37_0 ...\n",
            "reinstalling: hyperlink-18.0.0-py37_0 ...\n",
            "reinstalling: jedi-0.12.1-py37_0 ...\n",
            "reinstalling: more-itertools-4.3.0-py37_0 ...\n",
            "reinstalling: multipledispatch-0.6.0-py37_0 ...\n",
            "reinstalling: networkx-2.1-py37_0 ...\n",
            "reinstalling: nltk-3.3.0-py37_0 ...\n",
            "reinstalling: openpyxl-2.5.6-py37_0 ...\n",
            "reinstalling: packaging-17.1-py37_0 ...\n",
            "reinstalling: partd-0.3.8-py37_0 ...\n",
            "reinstalling: pathlib2-2.3.2-py37_0 ...\n",
            "reinstalling: pexpect-4.6.0-py37_0 ...\n",
            "reinstalling: pillow-5.2.0-py37heded4f4_0 ...\n",
            "reinstalling: pyasn1-modules-0.2.2-py37_0 ...\n",
            "reinstalling: pyqt-5.9.2-py37h05f1152_2 ...\n",
            "reinstalling: python-dateutil-2.7.3-py37_0 ...\n",
            "reinstalling: qtawesome-0.4.4-py37_0 ...\n",
            "reinstalling: setuptools-40.2.0-py37_0 ...\n",
            "reinstalling: singledispatch-3.4.0.3-py37_0 ...\n",
            "reinstalling: sortedcollections-1.0.1-py37_0 ...\n",
            "reinstalling: sphinxcontrib-websupport-1.1.0-py37_1 ...\n",
            "reinstalling: sympy-1.2-py37_0 ...\n",
            "reinstalling: terminado-0.8.1-py37_1 ...\n",
            "reinstalling: traitlets-4.3.2-py37_0 ...\n",
            "reinstalling: zict-0.1.3-py37_0 ...\n",
            "reinstalling: zope.interface-4.5.0-py37h14c3975_0 ...\n",
            "reinstalling: bleach-2.1.4-py37_0 ...\n",
            "reinstalling: clyent-1.2.2-py37_1 ...\n",
            "reinstalling: cryptography-2.3.1-py37hc365091_0 ...\n",
            "reinstalling: cython-0.28.5-py37hf484d3e_0 ...\n",
            "reinstalling: distributed-1.23.1-py37_0 ...\n",
            "reinstalling: get_terminal_size-1.0.0-haa9412d_0 ...\n",
            "reinstalling: gevent-1.3.6-py37h7b6447c_0 ...\n",
            "reinstalling: isort-4.3.4-py37_0 ...\n",
            "reinstalling: jinja2-2.10-py37_0 ...\n",
            "reinstalling: jsonschema-2.6.0-py37_0 ...\n",
            "reinstalling: jupyter_core-4.4.0-py37_0 ...\n",
            "reinstalling: navigator-updater-0.2.1-py37_0 ...\n",
            "reinstalling: nose-1.3.7-py37_2 ...\n",
            "reinstalling: pango-1.42.4-h049681c_0 ...\n",
            "reinstalling: pygments-2.2.0-py37_0 ...\n",
            "reinstalling: pytest-3.8.0-py37_0 ...\n",
            "reinstalling: wheel-0.31.1-py37_0 ...\n",
            "reinstalling: flask-1.0.2-py37_1 ...\n",
            "reinstalling: jupyter_client-5.2.3-py37_0 ...\n",
            "reinstalling: nbformat-4.4.0-py37_0 ...\n",
            "reinstalling: pip-10.0.1-py37_0 ...\n",
            "reinstalling: prompt_toolkit-1.0.15-py37_0 ...\n",
            "reinstalling: pylint-2.1.1-py37_0 ...\n",
            "reinstalling: pyopenssl-18.0.0-py37_0 ...\n",
            "reinstalling: pytest-openfiles-0.3.0-py37_0 ...\n",
            "reinstalling: pytest-remotedata-0.3.0-py37_0 ...\n",
            "reinstalling: secretstorage-3.1.0-py37_0 ...\n",
            "reinstalling: flask-cors-3.0.6-py37_0 ...\n",
            "reinstalling: ipython-6.5.0-py37_0 ...\n",
            "reinstalling: keyring-13.2.1-py37_0 ...\n",
            "reinstalling: nbconvert-5.4.0-py37_1 ...\n",
            "reinstalling: service_identity-17.0.0-py37h28b3542_0 ...\n",
            "reinstalling: urllib3-1.23-py37_0 ...\n",
            "reinstalling: ipykernel-4.9.0-py37_1 ...\n",
            "reinstalling: requests-2.19.1-py37_0 ...\n",
            "reinstalling: twisted-18.7.0-py37h14c3975_1 ...\n",
            "reinstalling: anaconda-client-1.7.2-py37_0 ...\n",
            "reinstalling: jupyter_console-5.2.0-py37_1 ...\n",
            "reinstalling: prometheus_client-0.3.1-py37h28b3542_0 ...\n",
            "reinstalling: qtconsole-4.4.1-py37_0 ...\n",
            "reinstalling: sphinx-1.7.9-py37_0 ...\n",
            "reinstalling: spyder-kernels-0.2.6-py37_0 ...\n",
            "reinstalling: anaconda-navigator-1.9.2-py37_0 ...\n",
            "reinstalling: anaconda-project-0.8.2-py37_0 ...\n",
            "reinstalling: notebook-5.6.0-py37_0 ...\n",
            "reinstalling: numpydoc-0.8.0-py37_0 ...\n",
            "reinstalling: jupyterlab_launcher-0.13.1-py37_0 ...\n",
            "reinstalling: spyder-3.3.1-py37_1 ...\n",
            "reinstalling: widgetsnbextension-3.4.1-py37_0 ...\n",
            "reinstalling: ipywidgets-7.4.1-py37_0 ...\n",
            "reinstalling: jupyterlab-0.34.9-py37_0 ...\n",
            "reinstalling: _ipyw_jlab_nb_ext_conf-0.1.0-py37_0 ...\n",
            "reinstalling: jupyter-1.0.0-py37_7 ...\n",
            "reinstalling: bokeh-0.13.0-py37_0 ...\n",
            "reinstalling: bottleneck-1.2.1-py37h035aef0_1 ...\n",
            "reinstalling: conda-4.5.11-py37_0 ...\n",
            "reinstalling: conda-build-3.15.1-py37_0 ...\n",
            "reinstalling: datashape-0.5.4-py37_1 ...\n",
            "reinstalling: h5py-2.8.0-py37h989c5e5_3 ...\n",
            "reinstalling: imageio-2.4.1-py37_0 ...\n",
            "reinstalling: matplotlib-2.2.3-py37hb69df0a_0 ...\n",
            "reinstalling: mkl_fft-1.0.4-py37h4414c95_1 ...\n",
            "reinstalling: mkl_random-1.0.1-py37h4414c95_1 ...\n",
            "reinstalling: numpy-1.15.1-py37h1d66e8a_0 ...\n",
            "reinstalling: numba-0.39.0-py37h04863e7_0 ...\n",
            "reinstalling: numexpr-2.6.8-py37hd89afb7_0 ...\n",
            "reinstalling: pandas-0.23.4-py37h04863e7_0 ...\n",
            "reinstalling: pytest-arraydiff-0.2-py37h39e3cac_0 ...\n",
            "reinstalling: pytest-doctestplus-0.1.3-py37_0 ...\n",
            "reinstalling: pywavelets-1.0.0-py37hdd07704_0 ...\n",
            "reinstalling: scipy-1.1.0-py37hfa4b5c9_1 ...\n",
            "reinstalling: bkcharts-0.2-py37_0 ...\n",
            "reinstalling: dask-0.19.1-py37_0 ...\n",
            "reinstalling: patsy-0.5.0-py37_0 ...\n",
            "reinstalling: pytables-3.4.4-py37ha205bf6_0 ...\n",
            "reinstalling: pytest-astropy-0.4.0-py37_0 ...\n",
            "reinstalling: scikit-image-0.14.0-py37hf484d3e_1 ...\n",
            "reinstalling: scikit-learn-0.19.2-py37h4989274_0 ...\n",
            "reinstalling: astropy-3.0.4-py37h14c3975_0 ...\n",
            "reinstalling: odo-0.5.1-py37_0 ...\n",
            "reinstalling: statsmodels-0.9.0-py37h035aef0_0 ...\n",
            "reinstalling: blaze-0.11.3-py37_0 ...\n",
            "reinstalling: seaborn-0.9.0-py37_0 ...\n",
            "reinstalling: anaconda-5.3.1-py37_0 ...\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Anaconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Anaconda3: /usr/local\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 4.5.11\n",
            "  latest version: 4.12.0\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c defaults conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs: \n",
            "    - cyvlfeat\n",
            "    - python==3.7\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    openssl-1.0.2u             |       h7b6447c_0         3.1 MB\n",
            "    mkl-service-2.3.0          |   py37he8ac12f_0          56 KB\n",
            "    tk-8.6.11                  |       h1ccaba5_0         3.2 MB\n",
            "    zlib-1.2.11                |       h7f8727e_4         125 KB\n",
            "    certifi-2021.10.8          |   py37h06a4308_2         156 KB\n",
            "    cyvlfeat-0.5.1             |   py37h975b26e_0         2.0 MB  menpo\n",
            "    python-3.7.0               |       h6e4f718_3        30.6 MB\n",
            "    mkl_random-1.1.1           |   py37h0573a6f_0         375 KB\n",
            "    numpy-base-1.19.2          |   py37hfa32c7d_0         5.2 MB\n",
            "    ca-certificates-2022.3.29  |       h06a4308_0         124 KB\n",
            "    pip-21.2.2                 |   py37h06a4308_0         2.0 MB\n",
            "    libedit-3.1.20210910       |       h7f8727e_0         191 KB\n",
            "    six-1.16.0                 |     pyhd3eb1b0_1          19 KB\n",
            "    xz-5.2.5                   |       h7b6447c_0         438 KB\n",
            "    vlfeat-0.9.20              |                1         199 KB  menpo\n",
            "    libffi-3.2.1               |    hf484d3e_1007          52 KB\n",
            "    numpy-1.19.2               |   py37h54aff64_0          21 KB\n",
            "    sqlite-3.33.0              |       h62c20be_0         2.0 MB\n",
            "    libgcc-ng-9.1.0            |       hdf63c60_0         8.1 MB\n",
            "    setuptools-58.0.4          |   py37h06a4308_0         979 KB\n",
            "    _libgcc_mutex-0.1          |             main           3 KB\n",
            "    wheel-0.37.1               |     pyhd3eb1b0_0          31 KB\n",
            "    mkl_fft-1.3.0              |   py37h54f3939_0         185 KB\n",
            "    mkl-2020.2                 |              256       213.9 MB\n",
            "    ncurses-6.3                |       h7f8727e_2         1.0 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       274.2 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "    _libgcc_mutex:   0.1-main                     \n",
            "    cyvlfeat:        0.5.1-py37h975b26e_0    menpo\n",
            "    vlfeat:          0.9.20-1                menpo\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "    ca-certificates: 2018.03.07-0                  --> 2022.3.29-h06a4308_0    \n",
            "    certifi:         2018.8.24-py37_1              --> 2021.10.8-py37h06a4308_2\n",
            "    libedit:         3.1.20170329-h6b74fdf_2       --> 3.1.20210910-h7f8727e_0 \n",
            "    libffi:          3.2.1-hd88cf55_4              --> 3.2.1-hf484d3e_1007     \n",
            "    libgcc-ng:       8.2.0-hdf63c60_1              --> 9.1.0-hdf63c60_0        \n",
            "    mkl:             2019.0-118                    --> 2020.2-256              \n",
            "    mkl-service:     1.1.2-py37h90e4bf4_5          --> 2.3.0-py37he8ac12f_0    \n",
            "    mkl_fft:         1.0.4-py37h4414c95_1          --> 1.3.0-py37h54f3939_0    \n",
            "    mkl_random:      1.0.1-py37h4414c95_1          --> 1.1.1-py37h0573a6f_0    \n",
            "    ncurses:         6.1-hf484d3e_0                --> 6.3-h7f8727e_2          \n",
            "    numpy:           1.15.1-py37h1d66e8a_0         --> 1.19.2-py37h54aff64_0   \n",
            "    numpy-base:      1.15.1-py37h81de0dd_0         --> 1.19.2-py37hfa32c7d_0   \n",
            "    openssl:         1.0.2p-h14c3975_0             --> 1.0.2u-h7b6447c_0       \n",
            "    pip:             10.0.1-py37_0                 --> 21.2.2-py37h06a4308_0   \n",
            "    python:          3.7.0-hc3d631a_0              --> 3.7.0-h6e4f718_3        \n",
            "    setuptools:      40.2.0-py37_0                 --> 58.0.4-py37h06a4308_0   \n",
            "    six:             1.11.0-py37_1                 --> 1.16.0-pyhd3eb1b0_1     \n",
            "    sqlite:          3.24.0-h84994c4_0             --> 3.33.0-h62c20be_0       \n",
            "    tk:              8.6.8-hbc83047_0              --> 8.6.11-h1ccaba5_0       \n",
            "    wheel:           0.31.1-py37_0                 --> 0.37.1-pyhd3eb1b0_0     \n",
            "    xz:              5.2.4-h14c3975_4              --> 5.2.5-h7b6447c_0        \n",
            "    zlib:            1.2.11-ha838bed_2             --> 1.2.11-h7f8727e_4       \n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "openssl-1.0.2u       | 3.1 MB    | : 100% 1.0/1 [00:00<00:00,  1.26it/s]               \n",
            "mkl-service-2.3.0    | 56 KB     | : 100% 1.0/1 [00:00<00:00, 12.69it/s]\n",
            "tk-8.6.11            | 3.2 MB    | : 100% 1.0/1 [00:00<00:00,  1.09it/s]               \n",
            "zlib-1.2.11          | 125 KB    | : 100% 1.0/1 [00:00<00:00, 18.04it/s]\n",
            "certifi-2021.10.8    | 156 KB    | : 100% 1.0/1 [00:00<00:00, 17.16it/s]\n",
            "cyvlfeat-0.5.1       | 2.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.23it/s]               \n",
            "python-3.7.0         | 30.6 MB   | : 100% 1.0/1 [00:06<00:00,  6.10s/it]               \n",
            "mkl_random-1.1.1     | 375 KB    | : 100% 1.0/1 [00:00<00:00,  8.95it/s]\n",
            "numpy-base-1.19.2    | 5.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.69s/it]               \n",
            "ca-certificates-2022 | 124 KB    | : 100% 1.0/1 [00:00<00:00, 23.02it/s]\n",
            "pip-21.2.2           | 2.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.07it/s]               \n",
            "libedit-3.1.20210910 | 191 KB    | : 100% 1.0/1 [00:00<00:00, 11.43it/s]\n",
            "six-1.16.0           | 19 KB     | : 100% 1.0/1 [00:00<00:00, 31.47it/s]\n",
            "xz-5.2.5             | 438 KB    | : 100% 1.0/1 [00:00<00:00,  6.27it/s]               \n",
            "vlfeat-0.9.20        | 199 KB    | : 100% 1.0/1 [00:00<00:00,  4.48it/s]               \n",
            "libffi-3.2.1         | 52 KB     | : 100% 1.0/1 [00:00<00:00, 22.47it/s]\n",
            "numpy-1.19.2         | 21 KB     | : 100% 1.0/1 [00:00<00:00, 27.65it/s]\n",
            "sqlite-3.33.0        | 2.0 MB    | : 100% 1.0/1 [00:00<00:00,  2.12it/s]               \n",
            "libgcc-ng-9.1.0      | 8.1 MB    | : 100% 1.0/1 [00:01<00:00,  1.58s/it]               \n",
            "setuptools-58.0.4    | 979 KB    | : 100% 1.0/1 [00:00<00:00,  2.31it/s]               \n",
            "_libgcc_mutex-0.1    | 3 KB      | : 100% 1.0/1 [00:00<00:00, 29.04it/s]\n",
            "wheel-0.37.1         | 31 KB     | : 100% 1.0/1 [00:00<00:00, 28.81it/s]\n",
            "mkl_fft-1.3.0        | 185 KB    | : 100% 1.0/1 [00:00<00:00, 14.77it/s]\n",
            "mkl-2020.2           | 213.9 MB  | : 100% 1.0/1 [00:51<00:00, 51.57s/it]                \n",
            "ncurses-6.3          | 1.0 MB    | : 100% 1.0/1 [00:01<00:00,  1.06s/it]               \n",
            "Preparing transaction: / \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs: \n",
            "    - cython\n",
            "    - numpy\n",
            "    - scipy\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    cython-0.29.28             |   py37h295c915_0         2.1 MB\n",
            "    conda-4.12.0               |   py37h06a4308_0        16.9 MB\n",
            "    scipy-1.6.2                |   py37h91f5cce_0        19.9 MB\n",
            "    conda-package-handling-1.8.1|   py37h7f8727e_0         954 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        39.8 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "    conda-package-handling: 1.8.1-py37h7f8727e_0 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "    conda:                  4.5.11-py37_0         --> 4.12.0-py37h06a4308_0 \n",
            "    cython:                 0.28.5-py37hf484d3e_0 --> 0.29.28-py37h295c915_0\n",
            "    scipy:                  1.1.0-py37hfa4b5c9_1  --> 1.6.2-py37h91f5cce_0  \n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "cython-0.29.28       | 2.1 MB    | : 100% 1.0/1 [00:01<00:00,  1.03s/it]               \n",
            "conda-4.12.0         | 16.9 MB   | : 100% 1.0/1 [00:06<00:00,  6.01s/it]               \n",
            "scipy-1.6.2          | 19.9 MB   | : 100% 1.0/1 [00:05<00:00,  5.34s/it]               \n",
            "conda-package-handli | 954 KB    | : 100% 1.0/1 [00:00<00:00,  4.45it/s]               \n",
            "Preparing transaction: | \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Cloning into '/cyvlfeat'...\n",
            "remote: Enumerating objects: 1097, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 1097 (delta 9), reused 23 (delta 7), pack-reused 1049\u001b[K\n",
            "Receiving objects: 100% (1097/1097), 1.38 MiB | 19.86 MiB/s, done.\n",
            "Resolving deltas: 100% (602/602), done.\n",
            "Obtaining file:///cyvlfeat\n",
            "Installing collected packages: cyvlfeat\n",
            "  Attempting uninstall: cyvlfeat\n",
            "    Found existing installation: cyvlfeat 0.5.1\n",
            "    Uninstalling cyvlfeat-0.5.1:\n",
            "      Successfully uninstalled cyvlfeat-0.5.1\n",
            "  Running setup.py develop for cyvlfeat\n",
            "Successfully installed cyvlfeat-0.7.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3985gb9aOypG"
      },
      "source": [
        "###  0-2: Connect to your Google Drive.\n",
        "\n",
        "It is required for loading the data.\n",
        "\n",
        "Enter your authorization code to access your drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKffRxrvDSJX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b643373-3df3-40ed-a4cd-481a09709913"
      },
      "source": [
        "# mount drive https://datascience.stackexchange.com/questions/29480/uploading-images-folder-from-my-system-into-google-colab\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bypm5tteROL"
      },
      "source": [
        "### 0-3: Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W88TOaCsxpfw"
      },
      "source": [
        "# Import libraries\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import cyvlfeat\n",
        "import time\n",
        "import scipy\n",
        "import multiprocessing"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xv7wrsXBO-w"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTq8GkOJBN4b"
      },
      "source": [
        "def euclidean_dist(x, y):\n",
        "    \"\"\"\n",
        "    :param x: [m, d]\n",
        "    :param y: [n, d]\n",
        "    :return:[m, n]\n",
        "    \"\"\"\n",
        "    m, n = x.shape[0], y.shape[0]    \n",
        "    eps = 1e-6 \n",
        "\n",
        "    xx = np.tile(np.power(x, 2).sum(axis=1), (n,1)) #[n, m]\n",
        "    xx = np.transpose(xx) # [m, n]\n",
        "    yy = np.tile(np.power(y, 2).sum(axis=1), (m,1)) #[m, n]\n",
        "    xy = np.matmul(x, np.transpose(y)) # [m, n]\n",
        "    dist = np.sqrt(xx + yy - 2*xy + eps)\n",
        "\n",
        "    return dist\n",
        "\n",
        "def read_img(image_path):\n",
        "    img = Image.open(image_path).convert('L')\n",
        "    img = img.resize((480, 480))\n",
        "    return np.float32(np.array(img)/255.)\n",
        "\n",
        "def read_txt(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        data = f.read()\n",
        "    return data.split()\n",
        "    \n",
        "def dataset_setup(data_dir):\n",
        "    train_file_list = []\n",
        "    val_file_list = []\n",
        "\n",
        "    for class_name in ['aeroplane','background','car','horse','motorbike','person']:\n",
        "        train_txt_path = os.path.join(data_dir, class_name+'_train.txt')\n",
        "        train_file_list.append(np.array(read_txt(train_txt_path)))\n",
        "        val_txt_path = os.path.join(data_dir, class_name+'_val.txt')\n",
        "        val_file_list.append(np.array(read_txt(val_txt_path)))\n",
        "\n",
        "    train_file_list = np.unique(np.concatenate(train_file_list))\n",
        "    val_file_list = np.unique(np.concatenate(val_file_list))\n",
        "\n",
        "    f = open(os.path.join(data_dir, \"train.txt\"), 'w')\n",
        "    for i in range(train_file_list.shape[0]):\n",
        "        data = \"%s\\n\" % train_file_list[i]\n",
        "        f.write(data)\n",
        "    f.close()\n",
        "\n",
        "    f = open(os.path.join(data_dir, \"val.txt\"), 'w')\n",
        "    for i in range(val_file_list.shape[0]):\n",
        "        data = \"%s\\n\" % val_file_list[i]\n",
        "        f.write(data)\n",
        "    f.close()\n",
        "\n",
        "def load_train_data(data_dir):\n",
        "    dataset_setup(data_dir)\n",
        "    num_proc = 12 # num_process\n",
        "\n",
        "    txt_path = os.path.join(data_dir, 'train.txt')\n",
        "    file_list = read_txt(txt_path)\n",
        "    image_paths = [os.path.join(data_dir+'/images', file_name+'.jpg') for file_name in file_list]\n",
        "    with multiprocessing.Pool(num_proc) as pool:\n",
        "      imgs = pool.map(read_img, image_paths)\n",
        "      imgs = np.array(imgs)\n",
        "      idxs = np.array(file_list)\n",
        "\n",
        "    return imgs, idxs\n",
        "\n",
        "def load_val_data(data_dir):\n",
        "    dataset_setup(data_dir)\n",
        "    num_proc = 12 # num_process\n",
        "\n",
        "    txt_path = os.path.join(data_dir, 'val.txt')\n",
        "    file_list = read_txt(txt_path)\n",
        "    image_paths = [os.path.join(data_dir+'/images', file_name+'.jpg') for file_name in file_list]\n",
        "    with multiprocessing.Pool(num_proc) as pool:\n",
        "      imgs = pool.map(read_img, image_paths)\n",
        "      imgs = np.array(imgs)\n",
        "      idxs = np.array(file_list)\n",
        "    \n",
        "    return imgs, idxs\n",
        "\n",
        "def get_labels(idxs, target_idxs):\n",
        "    \"\"\"\n",
        "    Get the labels from file index(name).\n",
        "\n",
        "    :param idxs(numpy.array): file index(name). shape:[num_images, ]\n",
        "    :param target_idxs(numpy.array): target index(name). shape:[num_target,]\n",
        "    :return(numpy.array): Target label(Binary label consisting of True and False). shape:[num_images,]\n",
        "    \"\"\"\n",
        "    return np.isin(idxs, target_idxs)\n",
        "\n",
        "def load_train_idxs(data_dir):\n",
        "    txt_path = os.path.join(data_dir, 'train.txt')\n",
        "    train_idxs = np.array(read_txt(txt_path))\n",
        "    return train_idxs\n",
        "\n",
        "def load_val_idxs(data_dir):\n",
        "    txt_path = os.path.join(data_dir, 'val.txt')\n",
        "    val_idxs = np.array(read_txt(txt_path))\n",
        "    return val_idxs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c5F-N9wfzZW"
      },
      "source": [
        "## Step 1: Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYFEIkL24nJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d0f7682-1f03-43af-c087-3837c058f10c"
      },
      "source": [
        "''' \n",
        "Set your data path for loading images & labels.\n",
        "Example) CS_DATA_DIR = '/gdrive/My Drive/data'\n",
        "'''\n",
        "\n",
        "%env CS_DATA_DIR=/gdrive/My Drive/data\n",
        "!mkdir -p \"$CS_DATA_DIR\"\n",
        "!cd \"$CS_DATA_DIR\" && wget http://www.di.ens.fr/willow/events/cvml2013/materials/practicals/category-level/practical-category-recognition-2013a-data-only.tar.gz && tar -zxf practical-category-recognition-2013a-data-only.tar.gz"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CS_DATA_DIR=/gdrive/My Drive/data\n",
            "--2022-04-05 10:04:27--  http://www.di.ens.fr/willow/events/cvml2013/materials/practicals/category-level/practical-category-recognition-2013a-data-only.tar.gz\n",
            "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.di.ens.fr/willow/events/cvml2013/materials/practicals/category-level/practical-category-recognition-2013a-data-only.tar.gz [following]\n",
            "--2022-04-05 10:04:27--  https://www.di.ens.fr/willow/events/cvml2013/materials/practicals/category-level/practical-category-recognition-2013a-data-only.tar.gz\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘practical-category-recognition-2013a-data-only.tar.gz.4’\n",
            "\n",
            "practical-category-     [           <=>      ] 964.15M  2.96MB/s    in 5m 25s  \n",
            "\n",
            "2022-04-05 10:09:53 (2.97 MB/s) - ‘practical-category-recognition-2013a-data-only.tar.gz.4’ saved [1010984641]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GW7H_2iPxzb"
      },
      "source": [
        "category = ['aeroplane', 'car', 'horse', 'motorbike', 'person'] # DON'T MODIFY THIS.\n",
        "data_dir = os.path.join(os.environ[\"CS_DATA_DIR\"], \"practical-category-recognition-2013a\", \"data\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX17mbhpXrNd"
      },
      "source": [
        "## Step 2: Bag of Visual Words (BoVW) Construction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QuLZmSxX2l5"
      },
      "source": [
        "### 2-1. (**Problem 1**): SIFT descriptor extraction & Save the descriptors (10pt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EWqpgbOV6yE"
      },
      "source": [
        "def SIFT_extraction(imgs):\n",
        "    \"\"\"\n",
        "    Extract Local SIFT descriptors from images using cyvlfeat.sift.sift().\n",
        "    Refer to https://github.com/menpo/cyvlfeat\n",
        "    You should set the parameters of cyvlfeat.sift.sift() as bellow.\n",
        "    1.compute_descriptor = True  2.float_descriptors = True\n",
        "\n",
        "    :param imgs(numpy.array): Gray-scale images in Numpy array format. shape:[num_images, width_size, height_size]\n",
        "    :return(numpy.array): SIFT descriptors. shape:[num_images, ], ndarray with object(descripotrs)\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    # cyvlfeat.sift.sift(image, ~, float_descriptors, compute_descriptor)\n",
        "    # ----- input -----\n",
        "    # image : [H, W] or [H, W, 1] `float32` `ndarray` A single channel, greyscale, `float32` numpy array (ndarray) representing the image to calculate descriptors for.\n",
        "    # float_descriptors : `bool`, optional If ``True``, the descriptor are returned in floating point rather than integer format.\n",
        "    # compute_descriptors : `bool`, optional If ``True``, the descriptors are also returned, as well as the keypoints (frames). computes the SIFT descriptors as well.  \n",
        "\n",
        "    # ----- returns -----\n",
        "    # frames : `(F, 4)` `float32` `ndarray` ``F`` is the number of keypoints (frames) used. This is the center of every dense SIFT descriptor that is extracted. \n",
        "              # frame format ``[Y, X, S, TH]``, where ``(Y, X)`` is the floating point center of the keypoint, ``S`` is the scale and ``TH`` is the orientation (in radians).\n",
        "    # descriptors : `(F, 128)` `uint8` or `float32` `ndarray`, optional ``F`` is the number of keypoints (frames) used. The 128 length vectors per keypoint extracted. ``uint8`` by default. Only returned if ``compute_descriptors=True``.\n",
        "\n",
        "    descriptor = []\n",
        "\n",
        "    for i in range(imgs.shape[0]):\n",
        "      frame, des = cyvlfeat.sift.sift(imgs[i].T, float_descriptors=True, compute_descriptor=True)\n",
        "      descriptor.append(des)\n",
        "      \n",
        "    return np.array(descriptor)\n",
        "    \n",
        "    # ----- analysis -----\n",
        "    # Returm Type: <class 'numpy.ndarray'>\n",
        "    # Return Size: [num_images, object]\n",
        "    # Object Size: [num_des_of_each_img, 128]; After all, since the number of num_des_of_each_img keeps changing, the number in the object keeps changing.\n",
        "    # Fianlly, Return Size: [num_images, num_des_of_each_img, 128]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmVgUvbVYS2x"
      },
      "source": [
        "### 2-2. (**Problem 2**): Codebook(Bag of Visual Words) construction (10pt)\n",
        "In this step, you will construct the codebook using K-means clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLFB9eaw95zo"
      },
      "source": [
        "def get_codebook(des , k):\n",
        "  \"\"\"\n",
        "  Construct the codebook with visual codewords using k-means clustering.\n",
        "  In this step, you should use cyvlfeat.kmeans.kmeans().\n",
        "  Refer to https://github.com/menpo/cyvlfeat\n",
        "\n",
        "  :param des(numpy.array): Descriptors.  shape:[num_images, ]\n",
        "  :param k(int): Number of visual words.\n",
        "  :return(numpy.array): Bag of visual words shape:[k, 128]\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  # cyvlfeat.kmeans.kmeans(data, num_centers)\n",
        "  # ----- input -----\n",
        "  # data : [N, D] `float32/float64` `ndarray` Input data to be clustered.\n",
        "  # num_centers : Number of clusters to compute\n",
        "\n",
        "  # ----- returns -----\n",
        "  # centers : [num_centers, D] `float32/float64` `ndarray` Computed clusters centers from the data points. the same dtype as ``data``. Also returns the cluster assignments.\n",
        "\n",
        "  # ----- process -----\n",
        "  # 1) convert \"des\" to \"collect_des\" following as: [num_images, num_des_of_each_img, 128] to [num_images * num_des_of_each_img, 128]\n",
        "  # 2) des shape like 1) is fit at kmeans data [N, D]\n",
        "  # 3) get codebook using cyvlfeat.kmeans.kmeans \n",
        "\n",
        "  collect_des = des[0]\n",
        "\n",
        "  for i in range(1, des.shape[0]):\n",
        "    collect_des = np.concatenate((collect_des, des[i]), axis=0)\n",
        "      \n",
        "  return cyvlfeat.kmeans.kmeans(collect_des, k)\n",
        "\n",
        "  # ----- analysis -----\n",
        "  # Returm Type: <class 'numpy.ndarray'>\n",
        "  # Return Size: [k, 128]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH92-UOaYiM2"
      },
      "source": [
        "### 2-3. (**Problem 3**): Encode images to histogram feature based on codewords (10pt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPQErulqCEKv"
      },
      "source": [
        "def extract_features(des, codebook):\n",
        "  \"\"\"\n",
        "  Construct the Bag-of-visual-Words histogram features for images using the codebook.\n",
        "  HINT: Refer to helper functions.\n",
        "\n",
        "  :param des(numpy.array): Descriptors.  shape:[num_images, ]\n",
        "  :param codebook(numpy.array): Bag of visual words. shape:[k, 128]\n",
        "  :return(numpy.array): Bag of visual words shape:[num_images, k]\n",
        "\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  # ----- input -----\n",
        "  # des shape : [num_images, num_des_of_each_img, 128]\n",
        "  # codebook shape : [num_of_visual_words(centers), 128]\n",
        "\n",
        "  # ----- returns -----\n",
        "  # BoW histogram : [num_images, BoW_histogram]\n",
        "\n",
        "  # ----- process -----\n",
        "  # 1) generate histogram = np.zeros(len(des.shape[0]), k)\n",
        "  # 2) measure the distance between codebooks for each image, and add +1 to the histogram index that matches the codebook index closest to the distance.\n",
        "  #    euclidean_dist(A, B) => A[m, d], B[n, d] => dist = [m, n] Print all distances between \"n\" per element of \"m\".\n",
        "  # 3) return the histogram\n",
        "\n",
        "  histogram = np.zeros((des.shape[0], codebook.shape[0]), \"float32\")\n",
        "\n",
        "  for i in range(des.shape[0]):\n",
        "    distance = euclidean_dist(des[i], codebook) # output: [num_des_of_each_img, k]\n",
        "\n",
        "    for j in range(distance.shape[0]):\n",
        "      smallest_index = np.where(distance[j] == np.amin(distance[j])) \n",
        "      histogram[i][smallest_index] += 1\n",
        "\n",
        "  return histogram\n",
        "\n",
        "  # ----- analysis -----\n",
        "  # Returm Type: <class 'numpy.float'>\n",
        "  # Return Size: [num_images, histogram(sum of frequency is k)]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJwCm_b3YwYe"
      },
      "source": [
        "## Step 3. (**Problem 4**): Train the classifiers (10pt)\n",
        "Train a classifier using the sklearn library (SVC) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9gOjAvXXGJy"
      },
      "source": [
        "from sklearn.svm import SVC"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkFInH3bDJPV"
      },
      "source": [
        "def train_classifier(features, labels, svm_params):\n",
        "  \"\"\"\n",
        "  Train the SVM classifier using sklearn.svm.svc()\n",
        "  Refer to https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "  :param features(numpy.array): Historgram representation. shape:[num_images, dim_feature]\n",
        "  :param labels(numpy.array): Target label(binary). shape:[num_images,]\n",
        "  :param svm_params(dict): parameters for classifier training.\n",
        "      ['C'](float): Regularization parameter.\n",
        "      ['kernel'](str): Specifies the kernel type to be used in the algorithm.\n",
        "  :return(sklearn.svm.SVC): Trained classifier\n",
        "  \"\"\"\n",
        "  # Your code here\n",
        "\n",
        "  # sklearn.svm.SVC(*, C=1.0, kernel='rbf', ~)\n",
        "  # ----- input -----\n",
        "  # C : float, default=1.0 Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\n",
        "  # kernel : {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’} or callable, default=’rbf’ Specifies the kernel type to be used in the algorithm. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape\n",
        "\n",
        "  # ----- returns -----\n",
        "  # trained_classifier model\n",
        "\n",
        "  # ---------------------------------------------------------------------\n",
        "\n",
        "  # sklearn.svm.SVC.fit(X, y)\n",
        "  # ----- input -----\n",
        "  # X : {array-like, sparse matrix} of shape (n_samples, n_features) or (n_samples, n_samples) Training vectors, where n_samples is the number of samples and n_features is the number of features. For kernel=”precomputed”, the expected shape of X is (n_samples, n_samples).\n",
        "  # y : array-like of shape (n_samples,) Target values (class labels in classification, real numbers in regression).\n",
        "\n",
        "  # ----- returns -----\n",
        "  # trained_classifier model that fitted estimator\n",
        "\n",
        "  trained_regularization = svm_params['C']\n",
        "  trained_kernel = svm_params['kernel']\n",
        "  trained_classifier = SVC(C=trained_regularization, kernel=trained_kernel)\n",
        "\n",
        "  return trained_classifier.fit(features, labels)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNvZlykjWfyn"
      },
      "source": [
        "def Trainer(feat_params, svm_params):\n",
        "    \n",
        "    \"\"\"\n",
        "    Train the SVM classifier.\n",
        "\n",
        "    :param feat_params(dict): parameters for feature extraction.\n",
        "        ['extractor'](function pointer): function for extrat local descriptoers. (e.g. SIFT_extraction, DenseSIFT_extraction, etc)\n",
        "        ['num_codewords'](int):\n",
        "        ['result_dir'](str): Diretory to save codebooks & results.\n",
        "        \n",
        "    :param svm_params(dict): parameters for classifier training.\n",
        "        ['C'](float): Regularization parameter.\n",
        "        ['kernel'](str): Specifies the kernel type to be used in the algorithm.\n",
        "   \n",
        "    :return(sklearn.svm.SVC): trained classifier\n",
        "    \"\"\"\n",
        "    \n",
        "    extractor = feat_params['extractor']\n",
        "    k = feat_params['num_codewords']\n",
        "    result_dir = feat_params['result_dir']\n",
        "    \n",
        "    if not os.path.isdir(result_dir):\n",
        "        os.mkdir(result_dir)\n",
        "    \n",
        "    print(\"Load the training data...\")\n",
        "    start_time = time.time()\n",
        "    train_imgs, train_idxs = load_train_data(data_dir)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "    \n",
        "    print(\"Extract the local descriptors...\")\n",
        "    start_time = time.time()\n",
        "    train_des = extractor(train_imgs)\n",
        "    np.save(os.path.join(result_dir, 'train_des.npy'), train_des)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "    \n",
        "    del train_imgs\n",
        "    \n",
        "    print(\"Construct the bag of visual words...\")\n",
        "    start_time = time.time()\n",
        "    codebook = get_codebook(train_des, k)\n",
        "    np.save(os.path.join(result_dir, 'codebook.npy'), codebook)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "\n",
        "    print(\"Extract the image features...\")\n",
        "    start_time = time.time()\n",
        "    train_features = extract_features(train_des, codebook)\n",
        "    np.save(os.path.join(result_dir, 'train_features.npy'), train_features)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "\n",
        "    del train_des, codebook\n",
        "    \n",
        "    print('Train the classifiers...')\n",
        "    accuracy = 0\n",
        "    models = {}\n",
        "    \n",
        "    for class_name in category:\n",
        "        target_idxs = np.array([read_txt(os.path.join(data_dir, '{}_train.txt'.format(class_name)))])\n",
        "        target_labels = get_labels(train_idxs, target_idxs)\n",
        "        \n",
        "        models[class_name] = train_classifier(train_features, target_labels, svm_params)\n",
        "        train_accuracy = models[class_name].score(train_features, target_labels) \n",
        "        print('{} Classifier train accuracy:  {:.4f}'.format(class_name ,train_accuracy))\n",
        "        accuracy += train_accuracy\n",
        "    \n",
        "    print('Average train accuracy: {:.4f}'.format(accuracy/len(category)))\n",
        "    del train_features, target_labels, target_idxs\n",
        "\n",
        "    return models"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkM12brUWjLs"
      },
      "source": [
        "feat_params = {'extractor': SIFT_extraction, 'num_codewords':1024, 'result_dir':os.path.join(data_dir,'sift_1024')}\n",
        "svm_params = {'C': 1, 'kernel': 'linear'}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0ULB-kc5jpk"
      },
      "source": [
        "- Below code will take about 30~70 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v_QngFiWlRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3488c798-de5b-4523-e21e-b6dfc3b3e6af"
      },
      "source": [
        "models = Trainer(feat_params, svm_params)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load the training data...\n",
            "46.3065 seconds\n",
            "Extract the local descriptors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500.3300 seconds\n",
            "Construct the bag of visual words...\n",
            "5021.6304 seconds\n",
            "Extract the image features...\n",
            "93.5468 seconds\n",
            "Train the classifiers...\n",
            "aeroplane Classifier train accuracy:  1.0000\n",
            "car Classifier train accuracy:  1.0000\n",
            "horse Classifier train accuracy:  1.0000\n",
            "motorbike Classifier train accuracy:  1.0000\n",
            "person Classifier train accuracy:  0.9555\n",
            "Average train accuracy: 0.9911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLnPCHFOalSk"
      },
      "source": [
        "## Step 4: Test the classifier on validation set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EN0ZUiXWoI3"
      },
      "source": [
        "def Test(feat_params, models):\n",
        "    \"\"\"\n",
        "    Test the SVM classifier.\n",
        "\n",
        "    :param feat_params(dict): parameters for feature extraction.\n",
        "        ['extractor'](function pointer): function for extrat local descriptoers. (e.g. SIFT_extraction, DenseSIFT_extraction, etc)\n",
        "        ['num_codewords'](int):\n",
        "        ['result_dir'](str): Diretory to load codebooks & save results.\n",
        "        \n",
        "    :param models(dict): dict of classifiers(sklearn.svm.SVC)\n",
        "    \"\"\"\n",
        "    \n",
        "    extractor = feat_params['extractor']\n",
        "    k = feat_params['num_codewords']\n",
        "    result_dir = feat_params['result_dir']\n",
        "    \n",
        "    print(\"Load the validation data...\")\n",
        "    start_time = time.time()\n",
        "    val_imgs, val_idxs = load_val_data(data_dir)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "    \n",
        "    print(\"Extract the local descriptors...\")\n",
        "    start_time = time.time()\n",
        "    val_des = extractor(val_imgs)\n",
        "    np.save(os.path.join(result_dir, 'val_des.npy'), val_des)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "    \n",
        "    \n",
        "    del val_imgs\n",
        "    codebook = np.load(os.path.join(result_dir, 'codebook.npy'))\n",
        "    \n",
        "    print(\"Extract the image features...\")\n",
        "    start_time = time.time()    \n",
        "    val_features = extract_features(val_des, codebook)\n",
        "    np.save(os.path.join(result_dir, 'val_features.npy'), val_features)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "\n",
        "    del val_des, codebook\n",
        "\n",
        "    print('Test the classifiers...')\n",
        "    accuracy = 0\n",
        "    for class_name in category:\n",
        "        target_idxs = np.array([read_txt(os.path.join(data_dir, '{}_val.txt'.format(class_name)))])\n",
        "        target_labels = get_labels(val_idxs, target_idxs)\n",
        "        \n",
        "        val_accuracy = models[class_name].score(val_features, target_labels)\n",
        "        print('{} Classifier validation accuracy:  {:.4f}'.format(class_name ,val_accuracy))\n",
        "        accuracy += val_accuracy\n",
        "    \n",
        "    del val_features, target_idxs, target_labels\n",
        "\n",
        "    print('Average validation accuracy: {:.4f}'.format(accuracy/len(category)))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z80KKyn7Ytfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4540d4b1-afd2-494d-c8ba-dd87386126b9"
      },
      "source": [
        "Test(feat_params ,models)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load the validation data...\n",
            "49.4729 seconds\n",
            "Extract the local descriptors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "423.2250 seconds\n",
            "Extract the image features...\n",
            "93.2445 seconds\n",
            "Test the classifiers...\n",
            "aeroplane Classifier validation accuracy:  0.9414\n",
            "car Classifier validation accuracy:  0.7498\n",
            "horse Classifier validation accuracy:  0.9038\n",
            "motorbike Classifier validation accuracy:  0.9054\n",
            "person Classifier validation accuracy:  0.5707\n",
            "Average validation accuracy: 0.8142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3X0BFLm756K"
      },
      "source": [
        "## **Problem 5**: Implement Dense SIFT (10pt)\n",
        "Modify the feature extractor using the dense SIFT and evaluate the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaY4kqQE8PXK"
      },
      "source": [
        "def DenseSIFT_extraction(imgs):\n",
        "  \"\"\"\n",
        "  Extract Dense SIFT descriptors from images using cyvlfeat.sift.dsift().\n",
        "  Refer to https://github.com/menpo/cyvlfeat\n",
        "  You should set the parameters of cyvlfeat.sift.dsift() as bellow.\n",
        "    1.step = 12  2.float_descriptors = True\n",
        "\n",
        "  :param train_imgs(numpy.array): Gray-scale images in Numpy array format. shape:[num_images, width_size, height_size]\n",
        "  :return(numpy.array): Dense SIFT descriptors. shape:[num_images, num_des_of_each_img, 128]\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  # cyvlfeat.sift.dsift(image, step, float_descriptors)\n",
        "  # ----- input -----\n",
        "  # image : Extracts a dense set of SIFT features from ``image``. [H, W] or [H, W, 1] `float32` `ndarray` A single channel, greyscale, `float32` numpy array (ndarray) representing the image to calculate descriptors for.\n",
        "  # step : `int`, optional A SIFT descriptor is extracted every ``step`` pixels. This allows for sub-sampling of the image.\n",
        "  # float_descriptors : `bool`, optional If ``True``, the descriptor are returned in floating point rather than integer format.\n",
        "\n",
        "  # ----- returns -----\n",
        "  # frames : `(F, 2)` or `(F, 3)` `float32` `ndarray` ``F`` is the number of keypoints (frames) used. This is the center of every dense SIFT descriptor that is extracted.\n",
        "  # descriptors : `(F, 128)` `uint8` or `float32` `ndarray` ``F`` is the number of keypoints (frames) used. The 128 length vectors per keypoint extracted. ``uint8`` by default.\n",
        "\n",
        "  descriptor = []\n",
        "\n",
        "  for i in range(imgs.shape[0]):\n",
        "    frame, des = cyvlfeat.sift.dsift(imgs[i].T, step=12, float_descriptors=True)\n",
        "    descriptor.append(des)\n",
        "      \n",
        "  return np.array(descriptor)\n",
        "    \n",
        "  # ----- analysis -----\n",
        "  # Returm Type: <class 'numpy.ndarray'>\n",
        "  # Return Size: [num_images, object]\n",
        "  # Object Size: [num_des_of_each_img, 128]; After all, since the number of num_des_of_each_img keeps changing, the number in the object keeps changing.\n",
        "  # Fianlly, Return Size: [num_images, num_des_of_each_img, 128]\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyhyW4yYEFxz"
      },
      "source": [
        "feat_params = {'extractor': DenseSIFT_extraction, 'num_codewords':1024, 'result_dir':os.path.join(data_dir,'dsift_1024')}\n",
        "svm_params = {'C': 1, 'kernel': 'linear'}"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPYn8ubgEuq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a59845f5-4980-4a7c-83e3-5e396ac5a348"
      },
      "source": [
        "models = Trainer(feat_params, svm_params)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load the training data...\n",
            "56.0977 seconds\n",
            "Extract the local descriptors...\n",
            "658.1388 seconds\n",
            "Construct the bag of visual words...\n",
            "9624.0328 seconds\n",
            "Extract the image features...\n",
            "173.3716 seconds\n",
            "Train the classifiers...\n",
            "aeroplane Classifier train accuracy:  1.0000\n",
            "car Classifier train accuracy:  1.0000\n",
            "horse Classifier train accuracy:  1.0000\n",
            "motorbike Classifier train accuracy:  1.0000\n",
            "person Classifier train accuracy:  0.9875\n",
            "Average train accuracy: 0.9975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b3X3gYHErl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3550810-b922-4255-d849-468c7f4c4798"
      },
      "source": [
        "Test(feat_params ,models)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load the validation data...\n",
            "55.1846 seconds\n",
            "Extract the local descriptors...\n",
            "636.3257 seconds\n",
            "Extract the image features...\n",
            "183.3848 seconds\n",
            "Test the classifiers...\n",
            "aeroplane Classifier validation accuracy:  0.9543\n",
            "car Classifier validation accuracy:  0.7947\n",
            "horse Classifier validation accuracy:  0.9074\n",
            "motorbike Classifier validation accuracy:  0.9103\n",
            "person Classifier validation accuracy:  0.5905\n",
            "Average validation accuracy: 0.8314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWyoU1yG7qhf"
      },
      "source": [
        "## **Problem 6**: Implement the Spatial Pyramid (10pt)\n",
        "Modify the feature extractor using the spatial pyramid matching and evaluate the performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Convert2dTo1d(des_2d):\n",
        "  row = des_2d.shape[0]\n",
        "  col = des_2d.shape[1]\n",
        "  dim = des_2d.shape[2]\n",
        "\n",
        "  des_1d = np.zeros((row * col, dim), \"float32\")\n",
        "\n",
        "  tmp = 0\n",
        "\n",
        "  for i in range(row):\n",
        "    for j in range(col):\n",
        "      des_1d[tmp, :] = des_2d[i, j, :]\n",
        "      tmp += 1\n",
        "  \n",
        "  return np.array(des_1d)"
      ],
      "metadata": {
        "id": "WzSp3Cz2Z-JL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJJpyKo98QQp"
      },
      "source": [
        "def SpatialPyramid(des, codebook):\n",
        "  \"\"\"\n",
        "  Extract image representation with Spatial Pyramid Matching using your DenseSIFT descripotrs & codebook.\n",
        "\n",
        "  :param des(numpy.array): DenseSIFT Descriptors.  shape:[num_images, num_des_of_each_img, 128]\n",
        "  :param codebook(numpy.array): Bag of visual words. shape:[k, 128]\n",
        "\n",
        "  :return(numpy.array): Image feature using SpatialPyramid [num_images, features_dim]\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  # ----- input -----\n",
        "  # des = [num_images, num_des_of_each_img, 128] ; getting all Dense descriptors of full images\n",
        "  # codebook = [k, 128] ; getting all codebook of full descriptors \n",
        "\n",
        "  # ----- returns -----\n",
        "  # SpatialPyramid histogram = [num_images, features_dim] ; getting all histogram representation of full images\n",
        "  # if resolution level is 2, then feature dimension following as: ((1 * 1) + (2 * 2) + (4 * 4)) * k = features_dim\n",
        "\n",
        "  # ----- process -----\n",
        "  # Assume part\n",
        "  #   1) set resolution level L; this case I set L = 2\n",
        "  #      In this case, L = 0 is same as above histogram matching / L = 1 get 2 x 2 grids (total = 4) / L = 2 get 4 x 4 grids (total = 16)\n",
        "  #   2) you must already know the \"image size\" and \"step size\" to divide the grid through this. In this function, it is assumed that you already know: So, it was specified directly.\n",
        "\n",
        "  # To do\n",
        "  #   1) convert des to des_2d (using \"image size\" and \"step size\") following as: des = [num_images, num_des_of_each_img, 128] to \"des_2d = [num_images, num_row_of_each_img, num_col_of_each_img, 128]\"\n",
        "  #   2) getting histogram until resolution level is 2 \n",
        "  #      2-1) define moving row(next_row) and col(next_col); this length decreases like \"40 -> 20 -> 10\"\n",
        "  #      2-2) define current_des_1d like = [num_of_des_in_current_bin_size, 128]; this shape decreases like \"[1600, 128] (1 time) -> [400, 128] (4 times) -> [100, 128] (16 times)\"\n",
        "  #      2-3) make tmp_histogram until same as bin size; this loop increases like \"1 -> 4 -> 16\"\n",
        "  #         2-3-1) find small distance between current_des_bin and codebook, finally tmp_histogram shape is [21, k]\n",
        "  #      2-4) make histogram like [image_num, 21 *k], and return this argument \n",
        "\n",
        "  img_width = 480\n",
        "  img_height = 480\n",
        "  img_num = des.shape[0]\n",
        "  des_dim = des.shape[2]\n",
        "  step_size = 12\n",
        "\n",
        "  des_col = int(img_width/step_size)\n",
        "  des_row = int(img_width/step_size)\n",
        "  des_2d = np.zeros((img_num, des_row, des_col, des_dim), \"float32\")\n",
        "\n",
        "  next_line = 0\n",
        "\n",
        "  for num in range(des_2d.shape[0]):\n",
        "    for row in range(des_2d.shape[1]):\n",
        "      for col in range(des_2d.shape[2]):\n",
        "        for dim in range(des_2d.shape[3]):\n",
        "          des_2d[num][row][col][dim] = des[num][next_line][dim]\n",
        "        next_line += 1\n",
        "    next_line = 0\n",
        "\n",
        "  L = 2\n",
        "  histogram = []\n",
        "\n",
        "  for num in range(img_num):\n",
        "    tmp_histogram = []\n",
        "\n",
        "    for l in range(L + 1):\n",
        "      next_row = int(des_2d.shape[1] / 2**l)\n",
        "      next_col = int(des_2d.shape[2] / 2**l)\n",
        "\n",
        "      current_bin_des = np.zeros(((next_row * next_col), des_2d.shape[3]), \"float32\")\n",
        "\n",
        "      x, y = 0, 0\n",
        "      \n",
        "      for row in range(2**l):\n",
        "        x = 0\n",
        "        for col in range(2**l):\n",
        "          current_bin_des = Convert2dTo1d(des_2d[num, y:y+next_row, x:x+next_col, :])\n",
        "\n",
        "          distance = euclidean_dist(current_bin_des, codebook) # output: [current_bin_size, codebook_num]\n",
        "          current_histogram = np.zeros(distance.shape[1], \"float32\")\n",
        "\n",
        "          for dis in range(distance.shape[0]):\n",
        "            smallest_index = np.where(distance[dis] == np.amin(distance[dis]))\n",
        "            current_histogram[smallest_index] += 1\n",
        "\n",
        "          weight = 2**(l - L)    \n",
        "          current_histogram = weight * current_histogram\n",
        "      \n",
        "          tmp_histogram.append(current_histogram)\n",
        "\n",
        "          x = x + next_col\n",
        "        y = y + next_row\n",
        "\n",
        "    tmp_histogram_1d = np.ravel(tmp_histogram)\n",
        "\n",
        "    histogram.append(tmp_histogram_1d)\n",
        "\n",
        "  return np.array(histogram)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAVKLXXyx2NE"
      },
      "source": [
        "def SP_Trainer(des_path, codebook_path, result_dir, svm_params):\n",
        "    \n",
        "    \"\"\"\n",
        "    Train the SVM classifier using SpatialPyramid representations.\n",
        "\n",
        "    :param des_path(str): path for loading training dataset DenseSIFT descriptors.\n",
        "    :param codebook(str): path for loading codebook for DenseSIFT descriptors.\n",
        "    :param result_dir(str): diretory to save features.\n",
        "        \n",
        "    :param svm_params(dict): parameters for classifier training.\n",
        "        ['C'](float): Regularization parameter.\n",
        "        ['kernel'](str): Specifies the kernel type to be used in the algorithm.\n",
        "   \n",
        "    :return(sklearn.svm.SVC): trained classifier\n",
        "    \"\"\"\n",
        "    train_idxs = load_train_idxs(data_dir)\n",
        "    train_des = np.load(des_path)\n",
        "    codebook = np.load(codebook_path)\n",
        "    train_features = SpatialPyramid(train_des, codebook)\n",
        "    np.save(os.path.join(result_dir, 'train_sp_features.npy'), train_features)\n",
        "\n",
        "    del train_des, codebook\n",
        "    \n",
        "    print('Train the classifiers...')\n",
        "    accuracy = 0\n",
        "    models = {}\n",
        "    \n",
        "    for class_name in category:\n",
        "        target_idxs = np.array([read_txt(os.path.join(data_dir, '{}_train.txt'.format(class_name)))])\n",
        "        target_labels = get_labels(train_idxs, target_idxs)\n",
        "        \n",
        "        models[class_name] = train_classifier(train_features, target_labels, svm_params)\n",
        "        train_accuracy = models[class_name].score(train_features, target_labels) \n",
        "        print('{} Classifier train accuracy:  {:.4f}'.format(class_name ,train_accuracy))\n",
        "        accuracy += train_accuracy\n",
        "    \n",
        "    print('Average train accuracy: {:.4f}'.format(accuracy/len(category)))\n",
        "    del train_features, target_labels, target_idxs\n",
        "\n",
        "    return models"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q--UT0fyEyc"
      },
      "source": [
        "def SP_Test(des_path, codebook_path, result_dir, models):\n",
        "    \"\"\"\n",
        "    Test the SVM classifier.\n",
        "\n",
        "    :param des_path(str): path for loading validation dataset DenseSIFT descriptors.\n",
        "    :param codebook(str): path for loading codebook for DenseSIFT descriptors.\n",
        "    :param result_dir(str): diretory to save features.      \n",
        "    :param models(dict): dict of classifiers(sklearn.svm.SVC)\n",
        "\n",
        "    \"\"\" \n",
        "    val_idxs = load_val_idxs(data_dir)\n",
        "    val_des = np.load(des_path)\n",
        "    codebook = np.load(codebook_path)\n",
        "    val_features = SpatialPyramid(val_des, codebook)\n",
        "    np.save(os.path.join(result_dir, 'val_sp_features.npy'), val_features)\n",
        "\n",
        "\n",
        "    del val_des, codebook\n",
        "\n",
        "    print('Test the classifiers...')\n",
        "    accuracy = 0\n",
        "    for class_name in category:\n",
        "        target_idxs = np.array([read_txt(os.path.join(data_dir, '{}_val.txt'.format(class_name)))])\n",
        "        target_labels = get_labels(val_idxs, target_idxs)\n",
        "        \n",
        "        val_accuracy = models[class_name].score(val_features, target_labels)\n",
        "        print('{} Classifier validation accuracy:  {:.4f}'.format(class_name ,val_accuracy))\n",
        "        accuracy += val_accuracy\n",
        "\n",
        "    del val_features, target_idxs, target_labels\n",
        "\n",
        "    print('Average validation accuracy: {:.4f}'.format(accuracy/len(category)))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS7Svvy2zTv_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4b619de-ce27-4361-f54a-bfcf659edc6d"
      },
      "source": [
        "#YOUR CODE HERE for training & testing with Spatial Pyramid\n",
        "\n",
        "# Training model using DenseSIFT and Spatial Pyramid\n",
        "des_path = os.path.join(data_dir, 'dsift_1024', 'train_des.npy')\n",
        "codebook_path = os.path.join(data_dir, 'dsift_1024', 'codebook.npy')\n",
        "result_dir = os.path.join(data_dir, 'dsift_1024')\n",
        "svm_params = {'C': 1, 'kernel': 'linear'}\n",
        "\n",
        "model = SP_Trainer(des_path, codebook_path, result_dir, svm_params)\n",
        "\n",
        "# Teset model using DenseSIFT and Spatial Pyramid\n",
        "des_path = os.path.join(data_dir, 'dsift_1024', 'val_des.npy')\n",
        "\n",
        "SP_Test(des_path, codebook_path, result_dir, model)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train the classifiers...\n",
            "aeroplane Classifier train accuracy:  1.0000\n",
            "car Classifier train accuracy:  1.0000\n",
            "horse Classifier train accuracy:  1.0000\n",
            "motorbike Classifier train accuracy:  1.0000\n",
            "person Classifier train accuracy:  1.0000\n",
            "Average train accuracy: 1.0000\n",
            "Test the classifiers...\n",
            "aeroplane Classifier validation accuracy:  0.9624\n",
            "car Classifier validation accuracy:  0.8832\n",
            "horse Classifier validation accuracy:  0.9487\n",
            "motorbike Classifier validation accuracy:  0.9507\n",
            "person Classifier validation accuracy:  0.6803\n",
            "Average validation accuracy: 0.8850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "401jsdB_8CA1"
      },
      "source": [
        "## **Problem 7**: Improve classification using non-linear SVM (10pt)\n",
        "Modify the classifier using the non-linear SVM and evaluate the performance. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg162rmJ8Q8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d86e5bb9-55c9-43b6-ea8e-9d68fde9502f"
      },
      "source": [
        "# ---------- Using kernel poly ----------\n",
        "# Training model using DenseSIFT and Spatial Pyramid\n",
        "des_path = os.path.join(data_dir, 'dsift_1024', 'train_des.npy')\n",
        "codebook_path = os.path.join(data_dir, 'dsift_1024', 'codebook.npy')\n",
        "result_dir = os.path.join(data_dir, 'dsift_1024')\n",
        "svm_params = {'C': 1, 'kernel': 'poly'}\n",
        "\n",
        "model = SP_Trainer(des_path, codebook_path, result_dir, svm_params)\n",
        "\n",
        "# Teset model using DenseSIFT and Spatial Pyramid\n",
        "des_path = os.path.join(data_dir, 'dsift_1024', 'val_des.npy')\n",
        "\n",
        "SP_Test(des_path, codebook_path, result_dir, model)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train the classifiers...\n",
            "aeroplane Classifier train accuracy:  0.9862\n",
            "car Classifier train accuracy:  0.8908\n",
            "horse Classifier train accuracy:  0.9551\n",
            "motorbike Classifier train accuracy:  0.9608\n",
            "person Classifier train accuracy:  0.7189\n",
            "Average train accuracy: 0.9023\n",
            "Test the classifiers...\n",
            "aeroplane Classifier validation accuracy:  0.9568\n",
            "car Classifier validation accuracy:  0.8601\n",
            "horse Classifier validation accuracy:  0.9394\n",
            "motorbike Classifier validation accuracy:  0.9483\n",
            "person Classifier validation accuracy:  0.5994\n",
            "Average validation accuracy: 0.8608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Using kernel rbf ----------\n",
        "# Training model using DenseSIFT and Spatial Pyramid\n",
        "des_path = os.path.join(data_dir, 'dsift_1024', 'train_des.npy')\n",
        "codebook_path = os.path.join(data_dir, 'dsift_1024', 'codebook.npy')\n",
        "result_dir = os.path.join(data_dir, 'dsift_1024')\n",
        "svm_params = {'C': 1, 'kernel': 'rbf'}\n",
        "\n",
        "model = SP_Trainer(des_path, codebook_path, result_dir, svm_params)\n",
        "\n",
        "# Teset model using DenseSIFT and Spatial Pyramid\n",
        "des_path = os.path.join(data_dir, 'dsift_1024', 'val_des.npy')\n",
        "\n",
        "SP_Test(des_path, codebook_path, result_dir, model)"
      ],
      "metadata": {
        "id": "11zuSnElqpYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Using kernel sigmoid ----------\n",
        "# Training model using DenseSIFT and Spatial Pyramid\n",
        "des_path = os.path.join(data_dir, 'dsift_1024', 'train_des.npy')\n",
        "codebook_path = os.path.join(data_dir, 'dsift_1024', 'codebook.npy')\n",
        "result_dir = os.path.join(data_dir, 'dsift_1024')\n",
        "svm_params = {'C': 1, 'kernel': 'sigmoid'}\n",
        "\n",
        "model = SP_Trainer(des_path, codebook_path, result_dir, svm_params)\n",
        "\n",
        "# Teset model using DenseSIFT and Spatial Pyramid\n",
        "des_path = os.path.join(data_dir, 'dsift_1024', 'val_des.npy')\n",
        "\n",
        "SP_Test(des_path, codebook_path, result_dir, model)"
      ],
      "metadata": {
        "id": "0GaGYqQ8qsxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b8Z-UnocePF"
      },
      "source": [
        "# <font color=\"blue\"> Discussion and Analysis </font>\n",
        "## Discussion Guidelines\n",
        "- You should write discussion about **Problem 5 ~ Problem 7**.\n",
        "- Simply reporting the results (e.g. classification accuracy) is not considered as a discussion.\n",
        "- For each problem's discussion, you should explain and compare how each method improves the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCn1KAvNc38r"
      },
      "source": [
        "Please write discussions on the results above."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **Problem 5 (Dense SIFT + Linear SVM)**\n",
        "- Problem 5 showed better performance than the original SIFT algorithm, which extracts a blob features and expresses each blob as a total of 8 vectors using magnitude and orientation. Train accuracy increased from **0.9911** to **0.9975**, and test accuracy increased from **0.8142** to **0.8314**. This is because sufficient image features were obtained by extracting descriptors from the entire image at regular intervals by extending the original SIFT method that detects features only at specific locations. That is, Dense SIFT provides enough feature measurements, so this result shows better performance than the original SIFT algorithm.\n",
        "\n",
        "- Since the image descriptor is densely extracted, it shows good performance, but there is still a problem in the process of inserting spatial information. If you have a well defined target object with clearly detectable and distinguishing features you can get good results, but you can't arbitrarily create these data.\n",
        "\n",
        "\n",
        "---\n",
        "### **Problem 6 (Dense SIFT + Spatial Pyramid + Linear SVM)**\n",
        "- The method extracted above Problem 5 does not provide spatial information, which is a problem with SIFT. Therefore, spatial information was inserted through the spatial pyramid method and performance was improved. First, the resolution level is decided. In this code, up to level 2 is implemented. Then, in level 0, descriptor vectors are extracted from one grid, in level 1, descriptor vectors for 4 (2x2) grids are extracted, and in level 2, descriptor vectors for 16 (4x4) grids are extracted. For each grid extracted in this way, a histogram is generated using the closest distance between the Bag of Visual Word (BoVW) and the descriptor of current grid, and spatial information about the image is included. At this time, the number k of codebooks, which is BoVW, is 1024. And the dimension of each image is as follows: (1 \\* 1) \\* 1024 + (2 \\* 2) \\* 1024 + (4 \\* 4) \\* 1024 = 21 \\* 1024 = 21504, so the total shape is [num_images, 21 \\* k], where k is 1024. In the existing method, information about the entire image was extracted and spatial information about it was added, and the result showed a sufficiently good performance result. We can see the training accuracy increased from **0.9975** to **1.0000**, and the test accuracy increased from **0.8314** to **0.8850**.\n",
        "\n",
        "---\n",
        "### **Problem 7 (Dense SIFT + Spatial Pyramid + Non-Linear SVM)**\n",
        "- SVM is an algorithm that finds a hyperplane to classify two categories well. However, it is not easy to obtain a linear hyperplane by actually representing data of images and performing SVM. So, it is made to find a linear hyperplane through mapping the data. This is called the Non-Linear SVM method, and among the functions provided in scikit-learn, the data are mapped to different spaces through the polynomial kernel, radial basis function kernel, and sigmoid kernel.\n",
        "\n",
        "- If you look at the results of SVM through data mapped in high dimension, you can see results that are worse or similar to those of Problem 6 (DenseSIFT + Spatial Pyramid + Linear SVM). The reason is that through the spatial pyramid, we have already acquired information in a higher dimension than the existing SIFT, and since it has already been overfitted for a small dataset, I think that performance does not come out beyond that. Therefore, I think that sufficient classification results can be obtained even using the DenseSIFT + Spatial Pyramid + Linear SVM method.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5XipY0ITi0Xt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "###**Problem 1 ~ 4 Result (SIFT + Linear SVM)**\n",
        "- Train Accuracy\n",
        "\n",
        "|Category|Accuracy|  \n",
        "|:---:|:---:|\n",
        "|aeroplane Classifier|1.0000|  \n",
        "|car Classifier|1.0000| \n",
        "|horse Classifier|1.0000|\n",
        "|motorbike Classifier|1.0000|\n",
        "|person Classifier|0.9555|\n",
        "|**Average**|**0.9911**|\n",
        "\n",
        "- Test Accuracy\n",
        "\n",
        "|Category|Accuracy|  \n",
        "|:---:|:---:|\n",
        "|aeroplane Classifier|0.9414|  \n",
        "|car Classifier|0.7498| \n",
        "|horse Classifier|0.9038|\n",
        "|motorbike Classifier|0.9054|\n",
        "|person Classifier|0.5707|\n",
        "|**Average**|**0.8142**|\n",
        "\n",
        "\n",
        "---\n",
        "###**Problem 5 Result (Dense SIFT + Linear SVM)**\n",
        "- Train Accuracy\n",
        "\n",
        "|Category|Accuracy|  \n",
        "|:---:|:---:|\n",
        "|aeroplane Classifier|1.0000|  \n",
        "|car Classifier|1.0000| \n",
        "|horse Classifier|1.0000|\n",
        "|motorbike Classifier|1.0000|\n",
        "|person Classifier|0.9875|\n",
        "|**Average**|**0.9975**|\n",
        "\n",
        "- Test Accuracy\n",
        "\n",
        "|Category|Accuracy|  \n",
        "|:---:|:---:|\n",
        "|aeroplane Classifier|0.9543|  \n",
        "|car Classifier|0.7947| \n",
        "|horse Classifier|0.9074|\n",
        "|motorbike Classifier|0.9103|\n",
        "|person Classifier|0.5905|\n",
        "|**Average**|**0.8314**|\n",
        "\n",
        "\n",
        "---\n",
        "###**Problem 6 Result (Dense SIFT + Spatial Pyramid + Linear SVM)**\n",
        "- Train Accuracy\n",
        "\n",
        "|Category|Accuracy|  \n",
        "|:---:|:---:|\n",
        "|aeroplane Classifier|1.0000|  \n",
        "|car Classifier|1.0000| \n",
        "|horse Classifier|1.0000|\n",
        "|motorbike Classifier|1.0000|\n",
        "|person Classifier|1.0000|\n",
        "|**Average**|**1.0000**|\n",
        "\n",
        "- Test Accuracy\n",
        "\n",
        "|Category|Accuracy|  \n",
        "|:---:|:---:|\n",
        "|aeroplane Classifier|0.9624|  \n",
        "|car Classifier|0.8832| \n",
        "|horse Classifier|0.9487|\n",
        "|motorbike Classifier|0.9507|\n",
        "|person Classifier|0.6803|\n",
        "|**Average**|**0.8850**|\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "###**Problem 7 Result (Dense SIFT + Spatial Pyramid + Non-Linear SVM)**\n",
        "- Train Accuracy\n",
        "\n",
        "|Category|Accuracy (poly)|Accuracy (rbf)| Accuracy (sigmoid)|  \n",
        "|:---:|:---:|:---:|:---:|\n",
        "|aeroplane Classifier|0.9862|0.9838|0.9668|\n",
        "|car Classifier|0.8908|0.9426|0.8774|\n",
        "|horse Classifier|0.9551|0.9575|0.9434|\n",
        "|motorbike Classifier|0.9608|0.9563|0.9511|\n",
        "|person Classifier|0.7189|0.9717|0.8184|\n",
        "|**Average**|**0.9023**|**0.9624**|**0.9114**|\n",
        "\n",
        "- Test Accuracy\n",
        "\n",
        "|Category|Accuracy (poly)|Accuracy (rbf)| Accuracy (sigmoid)|  \n",
        "|:---:|:---:|:---:|:---:|\n",
        "|aeroplane Classifier|0.9568|0.9551|0.9555|\n",
        "|car Classifier|0.8601|0.8690|0.8674|\n",
        "|horse Classifier|0.9394|0.9402|0.9365|\n",
        "|motorbike Classifier|0.9483|0.9495|0.9491|\n",
        "|person Classifier|0.5994|0.6964|0.6653|\n",
        "|**Average**|**0.8608**|**0.8821**|**0.8748**|\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yx6Gd79khnGi"
      }
    }
  ]
}